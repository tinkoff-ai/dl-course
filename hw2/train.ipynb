{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tinkoff.EDU-NLP-HW1.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "14mcMKUtMla4"
      },
      "source": [
        "!pip install cloud-tpu-client==0.10 torch==1.9.0 torchvision==0.10.0 torchtext==0.10.0 torchaudio==0.9.0 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.9-cp37-cp37m-linux_x86_64.whl\n",
        "!pip install catalyst==21.09\n",
        "!pip install datasets==1.11.0 transformers==4.10.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HdVSYp9GIPa"
      },
      "source": [
        "# BERT Embedding\n",
        "\n",
        "Привет! В этом домашнем задании вы обучите модель BERT для получения эмбеддингов предложений. Обучать BERT может потребовать много ресурсов, поэтому здесь предлагается использовать TPU в качестве ускорителя вычислений.\n",
        "\n",
        "!!!\n",
        "**Внимание**\n",
        "\n",
        "В этой работе запрещается использовать модель, которая была предобученная на **задачу эмбеддингов**. (Next Sentence Prediction разрешена)\n",
        "\n",
        "В этой работе запрещается использовать напрямую библиотеку **sentence-transformer**. \n",
        "\n",
        "!!!\n",
        "\n",
        "Статьи, которые помогут вам придумать улучшения для модели/процесса обучения/etc:\n",
        "- https://arxiv.org/pdf/1908.10084.pdf\n",
        "- https://ai.googleblog.com/2020/08/language-agnostic-bert-sentence.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fb7TyDAFMqT7"
      },
      "source": [
        "from torch import __version__ as torch_version\n",
        "from catalyst import __version__ as catalyst_version\n",
        "from catalyst import SETTINGS\n",
        "print(torch_version, catalyst_version, SETTINGS.xla_required)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-ypj12hF6cD"
      },
      "source": [
        "import transformers\n",
        "import datasets\n",
        "from datetime import datetime\n",
        "import hashlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, get_scheduler\n",
        "\n",
        "from catalyst import dl\n",
        "from catalyst.contrib.nn import ResidualBlock\n",
        "from catalyst.data import transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adMVP1pVJLfG"
      },
      "source": [
        "Тут описание обучения модели и получения эмбеддингов из модели. Тут не использована сила библиотеки transformers, этот код можно ускорить и упростить: https://github.com/huggingface/transformers/blob/c9d2cf855a38addaf036fa010e92cc8563a0ce07/src/transformers/models/bert/modeling_bert.py#L627\n",
        "\n",
        "\n",
        "Пояснение по коду:\n",
        "- forward используется для обучения модели\n",
        "- get_embedding используется для инференса модели\n",
        "\n",
        "В этом примере используется модель [RuBERT-tiny](https://huggingface.co/cointegrated/rubert-tiny). Можете попробовать любую другую предобученную модель.\n",
        "\n",
        "В некоторых моделях есть заранее предобученный pooler. Иногда полезно исползовать его. Можете менять код в ячейке ниже на свое усмотрение."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXfLjL1wIbcg"
      },
      "source": [
        "class SimBERT(nn.Module):\n",
        "    def __init__(self, model_name: str):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = transformers.BertModel.from_pretrained(model_name)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x_embs = self.model(input_ids=inputs[\"x_input_ids\"])\n",
        "        y_embs = self.model(input_ids=inputs[\"y_input_ids\"])\n",
        "\n",
        "        x_emb = self.pooling(x_embs)\n",
        "        y_emb = self.pooling(y_embs)\n",
        "\n",
        "        return nn.functional.cosine_similarity(x_emb, y_emb)\n",
        "\n",
        "    def get_embedding(self, x):\n",
        "        return self.pooling(self.model(**x))\n",
        "\n",
        "    def pooling(self, x):\n",
        "        return x[\"last_hidden_state\"][:, 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7lyRkiNKBG8"
      },
      "source": [
        "Тут находится Catalyst Runner для обучения модели. Он помогает работать с TPU также, как с GPU.\n",
        "\n",
        "Самые важные детали CustomRunner:\n",
        "\n",
        "- get_loaders -- Получение и обработки данных\n",
        "- get_optimizer/get_lr_sheduler -- Получение оптимизатора и lr_sheduler-а\n",
        "\n",
        "Здесь показано обучения не датасете [XNLI-ru](https://huggingface.co/datasets/xnli). Можете попробовать любой другой набор данных."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JHoycyNM8Mh"
      },
      "source": [
        "class CustomRunner(dl.IRunner):\n",
        "    def __init__(self, logdir):\n",
        "        super().__init__()\n",
        "        self._logdir = logdir\n",
        "\n",
        "    def get_engine(self):\n",
        "        return dl.DistributedXLAEngine()\n",
        "\n",
        "    def get_loggers(self):\n",
        "        return {\n",
        "            \"console\": dl.ConsoleLogger(),\n",
        "            \"csv\": dl.CSVLogger(logdir=self._logdir),\n",
        "            \"tensorboard\": dl.TensorboardLogger(logdir=self._logdir),\n",
        "        }\n",
        "\n",
        "    @property\n",
        "    def stages(self):\n",
        "        return [\"train\"]\n",
        "\n",
        "    def get_stage_len(self, stage: str) -> int:\n",
        "        return 1\n",
        "\n",
        "    def get_loaders(self, stage: str):\n",
        "        xnli =  datasets.load_dataset(\"xnli\", \"ru\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
        "\n",
        "        def process(examples):\n",
        "            x = tokenizer(\n",
        "                examples[\"premise\"],\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            y = tokenizer(\n",
        "                examples[\"hypothesis\"],\n",
        "                max_length=128,\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",\n",
        "            )\n",
        "            result = {f\"x_{k}\": v for k, v in x.items()}\n",
        "            result.update({f\"y_{k}\": v for k, v in y.items()})\n",
        "            return result\n",
        "\n",
        "        encoded_datasets = xnli.map(process, batched=True)\n",
        "        encoded_datasets = encoded_datasets.map(lambda x: {\"labels\": 1.0 - x[\"label\"]})\n",
        "        encoded_datasets.set_format(\n",
        "            type=\"torch\",\n",
        "            columns=[\n",
        "                \"x_input_ids\",\n",
        "                \"x_attention_mask\",\n",
        "                \"y_input_ids\",\n",
        "                \"y_attention_mask\",\n",
        "                \"labels\",\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        train_data = encoded_datasets[\"train\"]\n",
        "        valid_data = encoded_datasets[\"validation\"]\n",
        "\n",
        "        if self.engine.is_ddp:\n",
        "            train_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                train_data,\n",
        "                num_replicas=self.engine.world_size,\n",
        "                rank=self.engine.rank,\n",
        "                shuffle=True\n",
        "            )\n",
        "            valid_sampler = torch.utils.data.distributed.DistributedSampler(\n",
        "                valid_data,\n",
        "                num_replicas=self.engine.world_size,\n",
        "                rank=self.engine.rank,\n",
        "                shuffle=False\n",
        "            )\n",
        "        else:\n",
        "            train_sampler = valid_sampler = None\n",
        "\n",
        "        self.train_loader_len = len(DataLoader(train_data, batch_size=64, sampler=train_sampler))\n",
        "\n",
        "        return {\n",
        "            \"train\": DataLoader(train_data, batch_size=256, sampler=train_sampler),\n",
        "            \"valid\": DataLoader(valid_data, batch_size=64, sampler=valid_sampler),\n",
        "        }\n",
        "\n",
        "    def get_model(self, stage: str):\n",
        "        model = self.model \\\n",
        "            if self.model is not None \\\n",
        "            else SimBERT(\"cointegrated/rubert-tiny\")\n",
        "        return model\n",
        "\n",
        "    def get_criterion(self, stage: str):\n",
        "        return nn.MSELoss()\n",
        "\n",
        "    def get_optimizer(self, stage: str, model):\n",
        "        return torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "    def get_scheduler(self, stage: str, optimizer):\n",
        "        scheduler = get_scheduler(\n",
        "            \"linear\",\n",
        "            optimizer=optimizer,\n",
        "            num_warmup_steps=int(0.05 * self.train_loader_len) * self.stage_epoch_len,\n",
        "            num_training_steps=self.train_loader_len * self.stage_epoch_len\n",
        "        )\n",
        "        return scheduler\n",
        "\n",
        "    def get_callbacks(self, stage: str):\n",
        "        return {\n",
        "            \"criterion\": dl.CriterionCallback(\n",
        "                input_key=\"logits\", target_key=\"labels\", metric_key=\"loss\"\n",
        "            ),\n",
        "            \"optimizer\": dl.OptimizerCallback(metric_key=\"loss\"),\n",
        "            \"scheduler\": dl.SchedulerCallback(loader_key=\"valid\", metric_key=\"loss\", mode=\"batch\"),\n",
        "            \"checkpoint\": dl.CheckpointCallback(\n",
        "                self._logdir,\n",
        "                loader_key=\"valid\",\n",
        "                metric_key=\"loss\",\n",
        "                minimize=False,\n",
        "                save_n_best=1,\n",
        "            ),\n",
        "            \"tqdm\": dl.TqdmCallback(),\n",
        "        }\n",
        "\n",
        "    def handle_batch(self, batch):\n",
        "        outputs = self.model(batch)\n",
        "\n",
        "        self.batch = {\n",
        "            \"labels\": batch[\"labels\"],\n",
        "            \"logits\": outputs,\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nHkFlp88Rk-o"
      },
      "source": [
        "Запускаем логгер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETnv9u3_m1UB"
      },
      "source": [
        "logdir = f\"logs/{datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAN4vnmZRoy6"
      },
      "source": [
        "Запускаем обучение эмбеддингов."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ey6TLE3FRlbg"
      },
      "source": [
        "runner = CustomRunner(logdir)\n",
        "runner.run()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tw0B3Ty7Rtdg"
      },
      "source": [
        "Применняем обученные эмбеддинги на тестовой выборке."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZE5yA5Squ40"
      },
      "source": [
        "test_table = pd.read_csv(\"test.csv\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
        "\n",
        "model = runner.get_model(stage=1)\n",
        "embs = []\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    for _, row in tqdm(test_table.iterrows(), total=test_table.shape[0]):\n",
        "        x = tokenizer.batch_encode_plus(\n",
        "            [row[\"text\"]],\n",
        "            max_length=128,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "        emb = model.get_embedding(x)\n",
        "        embs.append(\n",
        "            {\n",
        "                \"id\": row[\"id\"],\n",
        "                \"emb\": emb.detach().cpu()[0].numpy()\n",
        "            }\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "id1IBfboR3XK"
      },
      "source": [
        "Для получения сабмита требуется дописать код ;-)\n",
        "Что осталось сделать:\n",
        "- Найти эмбеддинги для заголовков из fewshot.csv\n",
        "- Разметить остальные заголовки из test.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GV--Lzv1MRvl"
      },
      "source": [
        "# ваш код"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}